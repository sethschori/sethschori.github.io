{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello","text":"<p>I'm Seth and this is my digital garden and blog. My intention with this digital garden is to have a place to put information I learn and to build and share knowledge. Unlike blog posts, the entries in this learning garden won't be polished pieces of writing. In fact, if I'm successful at using my garden, there will always be incomplete entries because it will always be a work in progress. Given how blogs can often be oriented towards marketing and promotion, the temptation I'll try to avoid is worrying about how well something reads to others. In the garden, the primary audience is myself. However, posting it online will serve to share that information (and if I'm successful, knowledge) with others, as well as give me some accountability and connection in my learning journey.</p> <p>Some topics I may cultivate in my garden include: degrowth, post-capitalism, the solidarity economy, permaculture, and technology.</p> <p>My blog is currently a dozen technology-related articles from 2018-2019 when I was considering a career path in data science, so pretty dated stuff, but I figured I'd leave it as is. If I have future writing that feels more blog-like than garden-like then I'll stick it in the blog section.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2018/12/04/installing-anaconda-in-ubuntu/","title":"Installing Anaconda in Ubuntu","text":"","tags":["anaconda"]},{"location":"blog/2018/12/04/installing-anaconda-in-ubuntu/#why-i-chose-to-install-anaconda-in-ubuntu-linux","title":"Why I chose to install Anaconda in Ubuntu Linux","text":"<p>I want to start learning data science and there are a bunch of resources I want to try out (more on those another time). I watched the first two videos of Kevin Markham's Data School video series about pandas. But, in order to get going with pandas I needed to install it. He recommends using the Anaconda distribution of Python which is supposed to have the easiest package installation (numpy, pandas, etc.). From what I've read online, the ease of installation argument is less important for Linux machines where you have admin privileges than for, say, Windows machines where you don't. However, I figured I'd go with the recommended distribution so I could eliminate potential problems and also because I may end up working on a Windows machine later on, so I'd prefer the cross-platform ease of installation of Anaconda. </p>","tags":["anaconda"]},{"location":"blog/2018/12/04/installing-anaconda-in-ubuntu/#the-path-variable-problem-that-tripped-me-up","title":"The PATH variable problem that tripped me up","text":"<p>I found installing Anaconda to be really easy. I just went to the installation page, chose the Installing on Linux option and followed the instructions, including verifying the MD5 hash.</p> <p>I followed the FAQ's advice and let the installer add the path to Anaconda to my system's PATH variable. This works fine from Anaconda's perspective, but it changes the default system Python to the version of Python installed with Anaconda. In other words, if you type <code>python</code> at the command line, you'll be running the version of python in the Anaconda directory instead of the default directory (in my case, <code>/usr/bin/python</code>).</p> <p>After way too much time trying to resolve this, the solution I stumbled on turned out to be very simple. Here's the code block that Anaconda added at the end of my <code>~/.bashrc</code> file:</p> <pre><code># added by Anaconda3 5.3.1 installer\n# &gt;&gt;&gt; conda init &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$(CONDA_REPORT_ERRORS=false '/home/seth/anaconda3/bin/conda' shell.bash hook 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    \\eval \"$__conda_setup\"\nelse\n    if [ -f \"/home/seth/anaconda3/etc/profile.d/conda.sh\" ]; then\n        . \"/home/seth/anaconda3/etc/profile.d/conda.sh\"\n        CONDA_CHANGEPS1=false conda activate base\n    else\n        \\export PATH=\"/home/seth/anaconda3/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda init &lt;&lt;&lt;\n</code></pre> <p>The <code>\\export PATH=\"/home/seth/anaconda3/bin:$PATH\"</code> prepends the Anaconda directory to the beginning of your PATH variable's existing values. This is great, except that it means that when the system looks for the Python binaries, it finds them in the Anaconda directory and stops looking there.</p> <p>The solution I came up with (thanks to Piotr Dobrogost's comment on this Stack Overflow question) is to add the default Python directory to the beginning of the PATH variable, prior to the Anaconda directory.</p> <p>To accomplish that, here's the extra two lines of code that I added after the Anaconda code block:</p> <pre><code># added by Anaconda3 5.3.1 installer\n# &gt;&gt;&gt; conda init &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$(CONDA_REPORT_ERRORS=false '/home/seth/anaconda3/bin/conda' shell.bash hook 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    \\eval \"$__conda_setup\"\nelse\n    if [ -f \"/home/seth/anaconda3/etc/profile.d/conda.sh\" ]; then\n        . \"/home/seth/anaconda3/etc/profile.d/conda.sh\"\n        CONDA_CHANGEPS1=false conda activate base\n    else\n        \\export PATH=\"/home/seth/anaconda3/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda init &lt;&lt;&lt;\n\n# add /usr/bin to beginning of PATH so that python, python3, python2 use default system python not Anaconda python \nexport PATH=\"/usr/bin:$PATH\"\n</code></pre> <p>Is there a better, cleaner solution than this? Probably. I notice that this solution ends up putting <code>/usr/bin</code> in my PATH variable twice, which doesn't seem great. But at the moment this seems to be working, so I'm writing it up as a solution to this problem that I ran into.</p>","tags":["anaconda"]},{"location":"blog/2018/12/04/installing-anaconda-in-ubuntu/#useful-commands","title":"Useful commands","text":"<ul> <li><code>echo $PATH</code> will print the contents of the PATH variable to the screen. Note that directories are separated by colons.</li> <li><code>source ~/.bashrc</code> will run your <code>.bashrc</code> file and recreate your PATH variable, although note what the Anaconda FAQ has to say about this:</li> </ul> <p>If you have any terminal windows open, close them all then open a new one. You may need to restart your computer for the PATH change to take effect. </p>","tags":["anaconda"]},{"location":"blog/2018/12/04/installing-anaconda-in-ubuntu/#disclaimer","title":"Disclaimer","text":"<p>I'm definitely not a command line or bash expert. Your setup may be different than mine and so these instructions may not apply to you. Of course, if you notice anything that's wrong or could be improved in this post, I'd be glad to hear!</p>","tags":["anaconda"]},{"location":"blog/2018/12/06/getting-started-with-conda/","title":"Getting Started with conda","text":"<p>There are some good resources for getting started with conda on the conda website:</p> <ul> <li>conda cheat sheet</li> <li>Getting started with conda</li> </ul> <p>Here are a few commands that I found useful as a complete newbie to conda:</p> Command What it does <code>conda info</code> displays information about your setup <code>conda info --envs</code> lists your virtual environments <code>conda create --name jupyter --python=3.7</code> create a new virtual environment named <code>jupyter</code> with python version 3.7.x <code>conda create --name jupyter \"python&gt;=3.7\"</code> same as above except python version is greater than or equal to 3.7 (in this case, has the same effect as the previous command); note that you need to use double quotes when using <code>&gt;=</code> <code>conda activate jupyter</code> activate the virtual environment named jupyter; it's possible that your command might be different. You can also try <code>source activate jupyter</code> or <code>activate jupyter</code> in Windows. <code>conda deactivate</code> deactivate the current virtual environment; it's possible that your command might be different. You can also try <code>source deactivate</code> or <code>deactivate</code> in Windows. <code>conda env remove --name jupyter</code> delete the virtual environment named jupyter and everything in it <code>python -V</code> check which version of python you are using <code>which python</code> find the path to the version of python you are using","tags":["anaconda","conda"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/","title":"Getting Started with Jupyter notebook","text":"<p>Here are the steps I took to get started with Jupyter notebook:</p>","tags":["jupyter"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/#make-and-activate-a-new-conda-virtual-environment-with-python-3","title":"Make and activate a new conda virtual environment with Python 3","text":"<pre><code>conda create --name jupyter \"python&gt;=3\"\n</code></pre> <p>This creates a new conda virtual environment named <code>jupyter</code> with Python 3 installed. As of December 2018, the latest version is 3.7.1, which is the version that gets installed with the above command. And if the latest version when you're reading this happens to be 3.8.2, then you'll have 3.8.2 in your virtual environment.</p> <pre><code>conda activate jupyter\n</code></pre> <p>This activates the new <code>jupyter</code> virtual environment.</p>","tags":["jupyter"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/#configure-the-environment-to-allow-python-3-for-the-jupyter-notebooks","title":"Configure the environment to allow Python 3 for the Jupyter notebooks","text":"<p>When I first launched Jupyter, I found that I could only create Jupyter notebooks using Python 2. Python 2 is old and approaching the end of its life, so I wanted to be able to create Jupyter notebooks that use Python 3; plus, I prefer using Python 3's syntax.</p> <p>Thanks to this Stack Overflow answer, I discovered that a couple of more lines of configuration did the trick: </p> <pre><code>conda install notebook ipykernel\nipython kernel install --user\n</code></pre> <p>The first line installs the notebook and ipykernel packages into the active virtual environment (ipython is jupyter's old name).</p> <p>I don't really understand the second line, but I believe it has something to do with registering the ipython kernel to make it available to Jupyter, and the <code>--user</code> flag just means that you're doing it only for your user account.</p>","tags":["jupyter"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/#make-a-directory-and-optionally-initialize-git","title":"Make a directory and optionally initialize git","text":"<p>I think it's a good idea to have a directory for one's Jupyter notebooks so that they're in one place (unless you want to file them in different directories). It also allows you to then use git on the directory for version control.</p> <p>I made a directory called <code>jupyter</code>, but choose whatever directory name you want.</p> <pre><code>mkdir jupyter\ncd jupyter\n</code></pre> <p>You can also (optionally) initialize git in order to start using version control on the directory:</p> <pre><code>git init\n</code></pre> <p>I'm not going to cover how to use git because that's beyond the scope of this post, but there are some resources here on GitHub's site.</p>","tags":["jupyter"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/#start-the-jupyter-notebook-server-and-make-a-notebook","title":"Start the Jupyter notebook server and make a notebook","text":"<pre><code>jupyter notebook\n</code></pre> <p>This starts the server locally on your computer and automatically opens up a browser tab pointing to <code>http://localhost:8888/tree</code>, which is the default location and port (8888) for Jupyter notebooks.</p> <p>If you already have created any files or notebooks in the directory then you'll see them there, but if not you'll see an empty directory.</p> <p>In either case, you can make a new Jupyter notebook right in the browser. In the upper-right corner of the page you'll see a dropdown menu called <code>New</code>. If you click on that you should see an option to create a new notebook in whatever language(s) are available in this virtual environment. If you followed the directions above and they (hopefully) worked for you, then you should have an option to create a notebook using Python 3 and possibly Python 2. If you have other languages, you might see options to create notebooks in R, Julia, or perhaps other languages.</p>","tags":["jupyter"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/#start-using-the-notebook","title":"Start using the notebook","text":"<p>Your new notebook will be blank and look something like this:</p> <p></p> <p>Click on <code>Untitled</code> and give your notebook a name, such as <code>hello_world</code>.</p> <p>Click in the first blank, gray rectangle (called a cell) and enter some Python code, such as this:</p> <pre><code>squares = [num ** 2 for num in range(10)]\nprint(\"Hello world\")\nprint(f\"Here is a list of squares: {squares}\")\n</code></pre> <p>Type SHIFT-ENTER to execute the code. Your notebook should now look something like this:</p> <p></p> <p>Click the first icon which looks like a floppy disk to save your notebook (Jupyter will also autosave your notebook).</p> <p>You can now optionally commit your changes to version control if you'd like.</p> <p>Congratulations, you installed, configured, and created your first Jupyter notebook!</p>","tags":["jupyter"]},{"location":"blog/2018/12/19/data-science-survey-thoughts/","title":"Thoughts on JetBrains' 2018 Data Science Survey","text":"<p>As I\u2019m considering pursuing a career in data science, I found JetBrains 2018 Data Science Survey interesting because it gives me a sense (albeit an imperfect one) of which tools and technologies might be most useful to learn.</p> <p>Here are my takeaways from the survey:</p> <ul> <li>The most popular programming languages regularly used for data analysis are:</li> <li>Python 72%</li> <li>Java 62%</li> <li>R 23%</li> <li>As an aside, Kotlin runs on the Java Virtual Machine, integrates with Hadoop and Spark, and is more concise than Java. It is sponsored by JetBrains, and the survey acknowledges that it likely has some bias, but Kotlin may be an up-and-coming language.</li> <li>Spark is most popular for big data, followed closely by Hadoop. </li> <li>Jupyter notebooks and PyCharm are the most popular IDEs/editors. </li> <li>TensorFlow is the most popular deep learning library. (TensorFlow is lower-level than scikit-learn, according to these Quora answers.) </li> <li>Spreadsheet editors and Tableau are the most popular statistics packages for analyzing and visualizing data. </li> <li>The most popular operating systems are:</li> <li>Windows 62%</li> <li>Linux 44%</li> <li>macOS 37% </li> <li>Computations are performed on:</li> <li>local machines 78%</li> <li>clusters 36%</li> <li>cloud service 32%</li> <li>The most popular cloud services are:</li> <li>Amazon Web Services (AWS) 56%</li> <li>Google Cloud Platform 41%</li> <li>Microsoft Azure 28%</li> <li>The correlation seems to be that the more expertise one\u2019s manager has an data science, the more one tends to agree with this statement: \"My manager gives me realistic assignments that are relevant to my skills and responsibilities, with a clear and specific description of the requirements.\" </li> </ul> <p>It\u2019s nice that I already have experience with Python, Jupyter, PyCharm, spreadsheet editors, Windows and Linux, and AWS.</p> <p>I intend to next learn pandas. </p> <p>After that, my priorities would probably be: </p> <ul> <li>scikit-learn </li> <li>Spark (Hadoop?) </li> <li>TensorFlow </li> <li>Tableau </li> <li>Java </li> </ul>","tags":["meta data science"]},{"location":"blog/2019/01/01/pandas-tips/","title":"pandas Tips","text":"","tags":["pandas"]},{"location":"blog/2019/01/01/pandas-tips/#dot-notation-vs-bracket-notation-to-select-a-series","title":"Dot notation vs. bracket notation to select a Series","text":"<p>Each panda Series is essentially a column within a pandas DataFrame.  Whenever a column is added to a DataFrame, the column name (Series) is added  as an attribute on the pandas object. Series  can be accessed through either dot notation or bracket notation:</p> <pre><code># See this code snippet in context at: https://youtu.be/zxqjeyKP2Tk\nimport pandas as pd\nufo = pd.read_table('https://raw.githubusercontent.com/justmarkham/pandas-videos/master/data/ufo.csv',\n                   sep=',')\nufo_city = ufo['City']  # bracket notation\nufo_city = ufo.City  # dot notation\n</code></pre> <p>You must use bracket notation whenever there is:</p> <ul> <li>a space in the name of a Series</li> <li>a Series name that conflicts with a pandas method</li> </ul> <p>You must also use bracket notation when you want to make a new Series:</p> <pre><code># See this code snippet in context at: https://youtu.be/zxqjeyKP2Tk\nimport pandas as pd\nufo = pd.read_table('https://raw.githubusercontent.com/justmarkham/pandas-videos/master/data/ufo.csv',\n                   sep=',')\nufo['City_State'] = ufo['City'] + ', ' + ufo['State']\nufo.head()\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/pandas-tips/#keyboard-tricks-in-jupyter-notebooks","title":"Keyboard tricks in Jupyter notebooks","text":"<ul> <li>You can access all of an object's methods and attributes by typing  <code>&lt;name_of_the_pandas_object&gt;.</code> + <code>TAB-key</code>. (That's the name of the pandas  object, followed by a period and the tab key.)</li> <li>To see the optional or required parameters for a function or a method, when inside of the parentheses, hit <code>SHIFT</code> + <code>TAB</code> one, two, three, or four times.</li> </ul>","tags":["pandas"]},{"location":"blog/2019/01/01/pandas-tips/#a-few-helpful-methods-and-attributes","title":"A few helpful methods and attributes","text":"<ul> <li><code>.describe(include='all')</code> a method that returns statistics about a  DataFrame and produces results like this:</li> </ul> <p> - <code>.shape</code> an attribute that returns a tuple with the number of rows and  columns - <code>.dtypes</code> an attribute that returns the types of each of the Series in  the DataFrame, which produces results like this:</p> <p></p> <p>Note that -- just like in Python -- methods must be followed by <code>()</code> and  attributes must not.</p>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/","title":"Renaming and Removing Columns in pandas DataFrames","text":"","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#renaming-columns","title":"Renaming columns","text":"<p>Four methods for renaming columns in a pandas DataFrame:</p>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#1-rename-specific-columns","title":"1. Rename specific columns","text":"<p><code>foo.rename(columns={}, inplace=True)</code></p> <p>Pass a dict of columns to be renamed. For example:</p> <pre><code>foo.rename(columns={'old_col1': 'new_col1',\n                    'old_col2': 'new_col2'},\n                    inplace=True)\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#2-rename-all-the-column-names","title":"2. Rename all the column names","text":"<p>Set <code>.columns</code> to a list of all of the new column names. For example:</p> <pre><code>foo.columns = ['new_col1', 'new_col2']\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#3-rename-the-columns-when-reading-in-a-file","title":"3. Rename the columns when reading in a file","text":"<p>For example:</p> <pre><code>foo_new_names = ['new_col1', 'new_col2']\nfoo = pd.read_csv(data_file.csv,\n                  names=foo_new_names,\n                  header=0)\n</code></pre> <p>Note that in addition to setting the <code>names</code> parameter to a list of the new  column names, you must also set <code>header=0</code> to indicate that you're replacing  the existing column names in the 0th row (if the 0th row is a header row).</p>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#4-replace-existing-spaces-in-column-names-with-underscores","title":"4. Replace existing spaces in column names with underscores:","text":"<pre><code>foo.columns = foo.columns.str.replace(' ', '_')\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#removing-columns-and-rows","title":"Removing columns (and rows)","text":"<p>Here is the general syntax:</p> <pre><code>foo.drop(col_str_or_list_of_strs,\n         axis=1,  # 0 axis is rows, 1 axis is cols\n         inplace=True)\n</code></pre> <p>to drop a single column:</p> <pre><code>foo.drop('col_name',\n         axis=1,\n         inplace=True)\n</code></pre> <p>to drop multiple columns:</p> <pre><code>foo.drop(['col1', 'col2'],\n         axis=1,\n         inplace=True)\n</code></pre> <p>to drop a single row (data row 0):</p> <pre><code>foo.drop(0,  # int identifying the data row\n         axis=0,\n         inplace=True)\n</code></pre> <p>to drop multiple rows (data rows 1 and 2):</p> <pre><code>foo.drop([1, 2],  # list of ints identifying the data rows\n         axis=0,\n         inplace=True)\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/","title":"Sorting and Filtering in pandas","text":"","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#sorting","title":"Sorting","text":"","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#sorting-a-series-using-dot-notation","title":"Sorting a Series using dot notation","text":"<p>This is a simple as: <pre><code>foo.col_name.sort_values()\n</code></pre></p> <p>To sort in descending order: <pre><code>foo.col_name.sort_values(ascending=False)\n</code></pre></p> <p>Note that the <code>.sort_values()</code> method does not change the underlying sort order . It is a method on a Series that returns a sorted Series.</p>","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#sorting-a-dataframe-by-a-single-series","title":"Sorting a DataFrame by a single Series","text":"<p>This is even simpler than the first example!</p> <pre><code>foo.sort_values('col_name')\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#sorting-a-dataframe-by-multiple-series","title":"Sorting a DataFrame by multiple Series","text":"<p>Simply pass a list of the column names you want to sort by:</p> <pre><code>foo.sort_values(['col_name1', 'col_name2'])\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#filtering","title":"Filtering","text":"","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#filtering-rows-based-on-a-single-column-criteria","title":"Filtering rows based on a single column criteria","text":"<p>Filtering by a single column criteria is easy in pandas:</p> <pre><code>foo[foo.col_name &gt;= 42]\n</code></pre> <p>To understand how this works, please see my  Jupyter notebook about filtering,  which is based on  Data School's pandas video about filtering.</p>","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#filtering-rows-based-on-multiple-column-criteria","title":"Filtering rows based on multiple column criteria","text":"<p>Each criterion must be enclosed in parentheses and chained together with  either <code>&amp;</code> (AND) or <code>|</code> (OR). For example:</p> <pre><code>foo[(foo.col_name1 &gt;= 42) &amp; (foo.col_name2 == 'Bar')]\n</code></pre> <p>To use multiple OR criteria for the same column, you can use the <code>isin</code>  method. For example:</p> <pre><code>foo[(foo.col_name1 &gt;= 42) &amp; (foo.col_name2.isin(['Bar', 'Baz', 'Boz']))]\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/","title":"pandas Import and Iteration Tips","text":"<p>This post is based on my takeaways from a Q &amp; A video from Data School.</p>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#import-tips","title":"Import tips","text":"","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#reading-only-certain-columns-during-import-with-usecols-argument","title":"Reading only certain columns during import with <code>usecols</code> argument","text":"<p>The <code>usecols</code> argument can take a list of column names (<code>str</code>s) or column  positions (<code>int</code>s). For example:</p> <pre><code># column names\nfoo = pd.read_csv(data_file,\n                  usecols=['col_name1', 'col_name2'])\n</code></pre> <pre><code># column positions\nfoo = pd.read_csv(data_file,\n                  usecols=[0, 3])\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#reading-a-sample-of-rows-during-import-with-nrows-argument","title":"Reading a sample of rows during import with <code>nrows</code> argument","text":"<p>The <code>nrows</code> argument reads the first n rows of a file. For example, to only  import the first three rows from a file:</p> <pre><code>foo = pd.read_csv(data_file,\n                  nrows=3)\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#importing-columns-based-on-data-type-using-select_dtypes-include-and-numpy","title":"Importing columns based on data type using <code>.select_dtypes()</code>, <code>include</code>,  and numpy","text":"<p>The <code>select_dtypes()</code> method on a DataFrame allows you to select which  datatypes you want to keep in a DataFrame. The <code>include</code> argument on the  <code>select_dtypes()</code> method indicates which columns you want to include.  In this example, using numpy allows you to specify <code>np.number</code> which includes  number types <code>int64</code>, <code>float64</code>, etc.:</p> <pre><code>import numpy as np\nfoo = pd.read_csv(data_file)\nfoo = foo.select_dtypes(include=[np.number])\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#iteration-tips","title":"Iteration tips","text":"","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#iteration-through-a-series","title":"Iteration through a Series","text":"<p>This is incredibly easy, and just like iterating over any iterable in Python.  For example:</p> <pre><code>for city in foo.City:  # foo is a pandas DataFrame\n    print(city)\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#iteration-through-a-dataframe-with-iterrows","title":"Iteration through a DataFrame with <code>.iterrows()</code>","text":"<p>Using the DataFrame <code>.iterrows()</code> method is similar to <code>enumerate</code> in python:</p> <pre><code>for index, row in foo.iterrows():  # foo is a pandas DataFrame\n    print(index, row.City, row.State)\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/08/pandas-string-methods/","title":"pandas string methods","text":"<p>This blog post is based on lesson 12 (\"How do I use string methods in  pandas?\")  from Data School's  pandas video series.</p>","tags":["pandas"]},{"location":"blog/2019/01/08/pandas-string-methods/#pandas-has-many-string-methods-available-on-a-series-via-strsome_method","title":"pandas has many string methods available on a Series via <code>.str.some_method()</code>","text":"<p>For example: - <code>df.series_name.str.upper()</code> changes all the strings in the Series called  <code>series_name</code> (in the DataFrame called <code>df</code>) to uppercase - <code>df.series_name.str.title()</code> changes the strings to title case (first  character of each word is capitalized) - String methods on a Series return a Series. In the case of </p> <pre><code>df.series_name.str.contains('bar')\n</code></pre> <p>the <code>.contains()</code> method returns a Series of <code>True</code>s and <code>False</code>s, in which    <code>True</code> is returned if the string in the Series <code>series_name</code> contains <code>bar</code>    and <code>False</code> is returned if the string in the Series <code>series_name</code> does not    contain <code>bar</code>. - You could easily use the <code>True</code>/<code>False</code> Series returned by the <code>.contains()</code>  method above to filter a DataFrame. For example:</p> <pre><code>df[df.series_name.str.contains('bar')]\n</code></pre> <p>will return a new DataFrame filtered to only those rows in which the    <code>series_name</code> Series (aka the column called <code>series_name</code>) contains the    string <code>bar</code>.</p> <p>You can see all of the <code>str</code> methods available in the  pandas API reference.</p>","tags":["pandas"]},{"location":"blog/2019/01/08/pandas-string-methods/#string-methods-can-be-chained-together","title":"String methods can be chained together","text":"<p>For example:</p> <pre><code>df.series_name.str.replace('[', '').str.replace(']', '')\n</code></pre> <p>will operate on the Series called <code>series_name</code> in the DataFrame called <code>df</code>. The first <code>.replace()</code> method will replace <code>[</code> with nothing and the second <code>.replace()</code> method will replace <code>]</code> with nothing, allowing you to remove  the brackets from the strings in the Series.</p>","tags":["pandas"]},{"location":"blog/2019/01/08/pandas-string-methods/#many-pandas-string-methods-accept-regular-expressions","title":"Many pandas string methods accept regular expressions","text":"<p>The two chained <code>.replace()</code> methods in the previous example can be replaced  with a singular regex <code>.replace()</code>, like this:</p> <pre><code>df.series_name.str.replace('[\\[\\]]', '')\n</code></pre> <p>Here, the <code>.replace()</code> method is taking the <code>regex</code> string </p> <pre><code>'[\\[\\]]'\n</code></pre> <p>and replacing with nothing. That regular expression can be deconstructed as  follows:</p> <ul> <li>the outer brackets <code>[</code> and <code>]</code> define a character class, meaning that any  of the characters within those character class brackets will be replaced</li> <li>inside the outer brackets is <code>\\[\\]</code>. It represents the two characters  <code>[</code> and <code>]</code> which will be replaced. However, since brackets have a special  meaning in regular expressions, they need to be escaped with backslashes <code>\\</code>.  So the bracket characters to be replaced end up looking like this: </li> </ul> <pre><code>\\[\\]\n</code></pre> <p>You can see working code for all of the above examples in my  Jupyter notebook</p>","tags":["pandas"]},{"location":"blog/2019/01/08/pandas-vs-sql/","title":"pandas vs. SQL","text":"<p>As I'm starting to learn pandas, I noticed that it seems to have the  capabilities of doing everything that SQL can do, and more. So I was curious  about whether I would need SQL in a data science career, or if pandas could  suffice.</p> <p>A quick search turned up this article about PostgreSQL vs. pandas. My takeaway is that, not surprisingly, the situation is more complex than  simply one tool always being better than the other. Instead, SQL is best  suited for certain tasks, and vice versa for pandas.</p> <p>In particular, SQL is faster for typical database tasks, such as joins.  Whereas pandas is better for complex operations like string manipulation or  statistics.</p> <p>While, in theory, you could do practically everything with either tool, it  wouldn't be the most performant solution (pandas and SQL are each faster at  certain tasks, under certain situations), nor the most desirable  solution (e.g. SQL is a lingua franca and compatible with lots of  programming languages, so pandas could be limiting in that regard).</p> <p>The article, titled \"PostgreSQL vs. pandas\u200a\u2014\u200ahow to balance tasks between  server and client side\", offers these ten rules of thumb for evaluating whether to use SQL or pandas  for an analytics task:</p> <ol> <li>If doing a task in SQL can cut the amount of data returned to the client  (e.g. filtering to a smaller subset of data), then the task belongs on the  server.</li> <li>If the amount of data returned to the client remains unchanged or grows  (e.g. adding complex calculated columns; cross-joins, etc.) by doing it in   SQL, the task belongs into client side code.</li> <li>Test different setups on the server and client side to see which is more  efficient. In the worst case, you\u2019ll learn something.</li> <li>Never do in code what the SQL server can do well for you: Data  extraction  (CRUD, joins and set operations) &amp; simple data analysis.</li> <li>If it\u2019s painful or ugly, do it in client-side code: Complex data analysis  belongs into code. Leave formatting or math for the client side. The database  exists mainly to facilitate fast extraction of data.</li> <li>Minimise SQL complexity: Split overly complex, non-performant queries.  Two simpler queries will save the headache of maintaining one mega-query.  Alternatively, split it into a simple query and handle the complexity in  client-side code.</li> <li>Minimise database round trips: Try to do as much as you can in one  operation. Every semicolon is one round trip and adds another I/O operation.</li> <li>Configure your database carefully e.g. for postgres.  Otherwise, you default to sub-optimal algorithms which is expensive.</li> <li>It\u2019s well worth investing time in database schema optimisation.</li> <li>Same goes for setting optimal foreign/sort/distribution keys and  properly normalised tables to maintain the integrity of data.</li> </ol> <p>I think the full article is a worthwhile read.</p>","tags":["pandas"]},{"location":"blog/2019/01/23/next-data-science-steps/","title":"Next Data Science Steps and What I've Been Up To Recently","text":"<p>I haven't blogged in about two weeks. Here's what I've been up to with  data science and programming recently.</p> <ul> <li> <p>I had a small but interesting consulting project with a client I've  previously worked with. It involved a proof of concept to read and write  records via the APIs of a proprietary CRM system. It was an opportunity for  me to work with OAuth (OAuth2, actually) for the first time. I was  pleasantly surprised by how the requests library (which I've used before and liked) has built-in OAuth2 support via the requests-oauthlib package  and this made it much easier to work with OAuth2 than I expected. </p> </li> <li> <p>Here are the details on requests' support for Web Application  Flow (which is the flow I needed to use for this project).</p> </li> <li>And here is an article  from Digital Ocean about the different types of OAuth2 flows which was recommended to me by someone with more OAuth2 experience than I  have.</li> </ul> <p>This was also a chance for me to work directly with the API,    as there isn't a library available that I'm aware of. In contrast, when    I've previously worked with Salesforce's APIs I used the    <code>simple_salesforce</code> Python package, which worked well and which I was    grateful for, but it did introduce a layer of abstraction that led to a    slightly more superficial understanding.</p> <ul> <li> <p>I had another helpful informational interview with a data scientist doing  machine learning. From this, I realized that I want to test whether I'm interested in machine learning, because that will influence my learning path. He recommended Andrew's Ng's Machine Learning Coursera course. I respect this person's recommendation because he has many years of  programming experience, completed a data science masters degree at a  prestigious university, and is working in the industry. Although the course  material is fairly old, he thinks that Andrew explains the material really  well. It's a 55-hour course (free!) and I've recently gotten started on it.</p> </li> <li> <p>Finally, another client I've been assisting with work that's tangentially  related to programming has more directly involved me in programming.  I've been getting my toes wet with Ruby on Rails and it's my first time  working  with a production web app. It's been interesting trying to wrap my brain  around all the parts of a web app (still more to learn!) and to see how  things are set up at a startup where the founders are all accomplished  developers.</p> </li> </ul>","tags":["meta data science"]},{"location":"blog/2019/01/23/web-scraping-embedded-json/","title":"Web scraping a site with embedded JSON","text":"<p>code snippet</p> <pre><code>def scrape_list():\n    \"\"\"Scrape the cleantech list and put companies into a list of dicts.\"\"\"\n    cleantech100_url = 'https://i3connect.com/gct100/the-list'\n    cleantech100_base_url = 'https://i3connect.com'\n    request = requests.get(cleantech100_url)\n    bs = BeautifulSoup(request.content, \"html.parser\")\n    table = bs.table\n    # The HTML table has the headers: COMPANY, GEOGRAPHY, FUNDING, SECTOR,\n    # YEAR FOUNDED\n    header = [\n        'cleantech_url',     # from COMPANY\n        'company_country',   # from GEOGRAPHY\n        'company_funding',   # from FUNDING\n        'company_sector',    # from SECTOR\n        'company_year_founded',  # from YEAR FOUNDED\n        'company_region',    # Column exists but is not displayed in the header.\n        'company_video'      # Column exists but is not displayed in the header.\n    ]\n    companies = []\n    for row in table.tbody.find_all('tr'):\n        company = {}\n        index = 0\n        if 'id' in row.attrs and row.attrs['id'] == 'gct-table-no-results':\n            # Last row of table should be skipped because it's just got this:\n            # &lt;tr id=\"gct-table-no-results\"&gt;\n            #               &lt;td colspan=\"7\"&gt;No results found.&lt;/td&gt;\n            continue\n        for cell in row.find_all('td'):\n            # co_key is the key to use within company dict,\n            # e.g. company[co_key] could point to company['cleantech_url']\n            co_key = header[index]\n            if cell.string is None:\n                # The first and last columns of the HTML table have no text\n                # (cell.string is None).\n                try:\n                    # The first column of the HTML table holds a link to the\n                    # company detail page. This is handled by the try statement,\n                    # as there is an `href` attribute within the &lt;a&gt; tag.\n                    company[co_key] = cleantech100_base_url + cell.a.get('href')\n                except AttributeError:\n                    # The last column of the HTML table holds iframe links to\n                    # videos. Therefore, there is no &lt;a&gt; tag within the cell,\n                    # only a &lt;span&gt; element with a 'data-video-iframe' element.\n                    # This is handled by the except statement.\n                    video_url = cell.span.get('data-video-iframe')\n                    if len(video_url) &gt; 10:\n                        company[co_key] = video_url\n            else:\n                company[co_key] = cell.string\n            index += 1\n        companies.append(company)\n    return companies\n</code></pre>","tags":["meta data science"]},{"location":"clippings/2024-12-01-decade-of-building-massachusetts-solidarity-economy/","title":"A decade of building the Massachusetts solidarity economy","text":"<p>In Building the Solidarity Economy: A Decade\u2019s Assessment, educator, researcher, and activist Penn Loh takes stock of the past decade of effort to build a solidarity economy in the Boston area and Massachusetts. While I had heard about many of these projects and organizations from Penn\u2019s Solidarity Economy Movements course which I audited, I hadn\u2019t realized how so many of these organizations had been created in the past ten years.</p> <p>The article describes:</p> <ul> <li>New entities in the realms of food and land, such as new land trusts organized within the Greater Boston Community Land Trust Network, the Dorchester Food Co-op, and CERO composting.</li> <li>New funding support through Boston Impact Initiative and Boston Ujima Project, as well as $2M in participatory budgeting in Boston.</li> <li>The founding of a state office to support worker ownership: Massachusetts Center for Employee Ownership (MassCEO), which was likely to receive funding in the state budget, last I had seen.</li> <li>Efforts to fight the harms of the dominant economic system through the creation of permanently affordable housing. And while several groups have made impressive gains, I find the relatively small number of units sobering as to the level of work it takes to move housing out of the speculative market.</li> </ul> <p>The second half of the article looks at the challenges posed by the mindset and thought patterns embedded in the interconnected systems of oppression of \u201ccapitalism, heteropatriarchy, and imperialism\u201d. For example, funders had hoped that projects would be ready to fund in a linear fashion, which didn\u2019t happen at first, and there is a broader need to shift culture, learn to make decisions together non-hierarchically, and cultivate solidarity in all relations. The way forward will include cooperatives, but as Aaron Tanaka says, \u201cwe can\u2019t \u2018co-op our way out of capitalism\u2019 because the problems are not just economic.\u201d </p> <p>The article closes with a quote from Jasmine Gomez (which references Audre Lorde\u2019s essay \u201cThe Master's Tools Will Never Dismantle the Master's House\u201d) describing how the resources of the dominant system can be used to break down that system, reduce the harms of that system, or build a new system:</p> <p>[T]he master\u2019s tools \u2026 are what we have. Some people use them to try and dismantle. Some people use them to try and extend the life of systems so that they don\u2019t hurt our communities as much as they can, and I think some people use those tools to try and build something completely new. Whether or not we can is the experiment. And in that process, the healing and the relationship building is something that will come, that are not the tools of the masters.</p>"},{"location":"clippings/2024-12-01-zapatismo-30th-anniversary/","title":"Zapatismo\u2019s 30th anniversary","text":"<p>I just read Zapatismo at 30: An Indigenous Rights Movement Faces Perilous Times. Their seven principles of \u201cobeying by following,\u201d, stood out for me (further described in English here):</p> <ol> <li>To obey, not command</li> <li>To propose, not impose</li> <li>To represent, not supplant</li> <li>To convince, not conquer</li> <li>To construct, not destroy</li> <li>To serve others, not serve oneself</li> <li>To work from below, not seek to rise</li> </ol> <p>Despite the rise of the political left in Mexico, the Zapatistas face significant challenges today from drug cartels and displacement.</p> <p>I found the article particularly useful in describing what the Zapatistas have accomplished locally and what they have inspired internationally.</p>"},{"location":"degrowth/definitions-of-degrowth/","title":"Definitions of degrowth","text":"<p>Jason Hickel\u2019s Degrowth and MMT: A thought experiment blog post has this succinct definition of degrowth:</p> <p>Degrowth has two parts: an ecology part and a social justice part. It seeks to (a) reduce excess resource and energy use (specifically in high-income nations) in order to bring the economy back into balance with the living world, and (b) to do so while at the same time reducing inequality and improving people\u2019s access to the things they need to live long, healthy, flourishing lives.</p>"},{"location":"degrowth/quotes/","title":"Quotes","text":"<p>From Jason Hickel\u2019s blog post, A response to Pollin and Chomsky: We need a Green New Deal without growth:</p> <p>Growth is ultimately a kind of propaganda term; it takes processes of extraction, commodification and elite accumulation, which are quite often destructive to human communities and to ecology, and sells them as natural, good and common-sense (who could possibly be against growth?). The language of growth is the bedrock of capitalism\u2019s cultural hegemony.</p>"},{"location":"degrowth/terminology/","title":"Terminology","text":"<p>I\u2019ve wondered about the difference between the terms \u201cpost-growth\u201d and \u201cdegrowth\u201d and found this article: Post-growth, degrowth, the doughnut and circular economy: a short guide (originally posted at planetamateur.com). Here are some of my takeaways mixed with my own thoughts:</p> <ul> <li>The circular economy contrasts with the linear economy, which is a one-way path that extracts resources, which are then used and thrown away. The circular economy tries to close loops (William McDonough\u2019s term \u201ccradle to cradle\u201d comes to mind). This can be done by making waste into resource inputs, both at the larger scale of manufacturing (I recall an actual business that uses waste coffee grounds to grow edible mushrooms), to the smaller scale of things like repair cafes.</li> <li>The doughnut or doughnut economy is Kate Raworth\u2019s term, illustrated below, which ties together meeting human needs across a dozen categories (avoiding shortfalls) while staying within nine types of ecological limits (avoiding overshoot). </li> <li>The article describes post-growth as an \u201capproach without a toolkit.\u201d Post-growth is the simple (and obvious) recognition that we can\u2019t endlessly expand production and consumption on a finite planet. As I\u2019ve been reading in more detail in Kohei Saito\u2019s  Slow Down, absolute decoupling (in which the economy grows while material consumption does not) is a nice concept to dream about, but is fantasy. Post-growth moves beyond seeking green technologies as the solution to our environmental problems to calling into question growth itself. (Green technologies, while certainly useful and important, will never be a sufficient solution to our environmental problems as long as we have endless growth.) I also think of post-growth as somewhat synonymous with the term \u201csteady-state economy.\u201d Both of these terms, to me, describe a future state that we intend to reach, but they don\u2019t describe how to get there.</li> <li>In contrast, degrowth is the \u201cagenda of action\u201d that can get us to the targets that the doughnut tool can define. I think of degrowth as a strategy, or agenda, for reaching a post-growth (or steady-state) economy. Unlike the other terms, degrowth brings in a critical component of justice instead of focusing solely on the environment. Jason Hickel and Kohei Saito both discuss the need to address the dramatic inequality between the Global North and the Global South. Hickel discusses increasing consumption and development in the Global South while decreasing production in the Global North of extractive and destructive production that\u2019s less necessary, such as fast fashion, industrial beef, McMansions, advertising, military armaments, planned obsolescence, food waste, etc.</li> </ul> <p>Circular economy, doughnut economy, post-growth, and degrowth can be complementary. For example, </p>"},{"location":"other/2024-election-ending/","title":"2024 Election: Final Days and Next Steps","text":""},{"location":"other/2024-election-ending/#phone-banks-through-election-day-get-out-the-vote","title":"Phone Banks Through Election Day: Get Out the Vote","text":"<p>The election is trending towards Democrats, but it\u2019s definitely not a sure thing. So we need to do all we can to keep turning out voters through Election Day. Here are some opportunities:</p> <ul> <li>Call to get out the vote in Wisconsin and Pennsylvania with Swing Blue Alliance. Sign up for two-hour shifts on Sat 11/2, Sun 11/3, or Mon 11/4 between 12:00 - 8:00 PM Eastern.</li> <li>Call to get out the vote in Charlotte/Mecklenburg County, North Carolina with Swing Blue Alliance. Sign up for two-hour shifts on Sat 11/2 through Tue 11/5.</li> <li>Call White voters in battleground states whose real interests are aligned with the working class, but whom MAGA is trying to peel off through division and hatred. Join SURJ\u2019s Many Over MAGA Phonebanks! Sign up for shifts on Sat 11/2 through Tue 11/5.</li> <li>Call Black voters in six different states with Center for Common Ground, encouraging them to vote and providing resources that make it easier to check where they are registered and find the voting information they need. Sign up for any amount of time on Sun 11/3 between 12:00 - 8:00 PM Eastern.</li> <li>Call Black voters in Florida with Center for Common Ground, encouraging them to vote and providing resources that make it easier to check where they are registered and find the voting information they need. Sign up for a two-hour shift  on Mon 11/4 from 4:00 - 6:00 PM Eastern.</li> <li>Fight voter suppression in North Carolina by phone banking to rural voters of color with Center for Common Ground and the Workers Circle. We\u2019ll be calling to learn which issues are most important to them, help them make a voting plan, and know the voter ID they need to bring. Sign up for Mon 11/4 from 6:00 - 8:00 PM.</li> <li>Make calls to get out the vote for critical Senate candidates:<ul> <li>Sen. Jon Tester, Montana \u2014 sign up for shifts on Sat 11/2 through Tue 11/5 nearly anytime.</li> <li>Sen. Tammy Baldwin and Harris/Walz, Wisconsin \u2014 sign up for shifts on Sat 11/2 through Tue 11/5 between 11:00 AM - 9:00 PM Eastern.</li> <li>Sen. Bob Casey, Harris/Walz, and down ballot candidates, Pennsylvania \u2014 sign up for shifts on Sat 11/2 through Tue 11/5 between 9:00 AM - 9:00 PM Eastern.</li> </ul> </li> <li>Sign up to volunteer directly with the Harris/Walz campaign \u2014 online or in person \u2014 at go.kamalaharris.com.</li> </ul>"},{"location":"other/2024-election-ending/#phone-banks-post-election-day-cure-absentee-ballots","title":"Phone Banks Post-Election Day: \u201cCure\u201d Absentee Ballots","text":"<p>This election will be very close and every vote must be counted. Vote/ballot curing is the process of helping voters who have had their absentee ballot rejected by their local clerk for an issue with the signature or address. If these voters don\u2019t take steps to fix or \u201ccure\u201d their ballots, their votes will not be counted! You\u2019ll call these voters to inform them of the problem and guide them to cure their ballot.</p> <ul> <li>Arizona phone bank, sign up for shifts from Wed 11/6 through Sat 11/9</li> <li>Michigan phone bank, sign up for the shift on Wed 11/6 from 6:00 - 8:00 PM Eastern</li> <li>Nevada phone bank, sign up for shifts from Wed 11/6 through Tue 11/12</li> <li>North Carolina trainings for future shifts, sign up for Wed 11/6 from 12:00 - 1:00 or 7:00 - 8:00 Eastern</li> <li>Across the battlegrounds with Harris/Walz to the states that need help most, sign up for shifts on Wed 11/6, Sat-Mon 11/9-11/11, and Wed 11/13</li> </ul>"},{"location":"other/2024-election-ending/#plug-in-post-election-day-next-steps-and-analysis","title":"Plug In Post-Election Day: Next Steps and Analysis","text":"<p>This will likely be a nail biter past Election Day. Here are some opportunities to make meaning of the moment, connect with others, and hear about next steps regardless of who wins:</p> <ul> <li>Movement Voter PAC Briefing: What Just Happened?! \u2014 Join Movement Voter PAC on Zoom to jump for joy, cry bitter tears, or bite our nails awaiting delayed results in the company of like-minded Democratic and progressive donors, activists, organizers, and strategists. Sign up for Wed 11/6 at 3:00 PM Eastern.</li> <li>White People\u2019s Work After the Election with Showing Up For Racial Justice (SURJ) \u2014 No matter what happens on November 5, there will be important work for all of us to do in the days following. Join SURJ, the largest US organization that explicitly organizes white people for racial, economic and gender justice, for immediate calls to action to ensure every vote is counted and certified. White communities, organized by MAGA forces, are in the way of us winning on every issue we care about. Win or lose in this election, white people will have an important responsibility and role to play to reject Trump\u2019s plays for white support \u2014 and to bring thousands more white people into the multiracial movement we need to win. Sign up for Wed 11/6, 8:00 PM Eastern.</li> <li>Drop-in sessions with Swing Blue Alliance: SBA is holding post-election drop-in Zoom sessions for three days after the election. Volunteers from the SBA leadership team will be there to offer support, answer questions, celebrate our successes, and provide community. Sign up for drop-ins on Wed 11/6 through Fri 11/8, 1:00-2:00 PM or 6:00-7:00 PM Eastern.</li> <li>Movement for Black Lives Post Election Virtual Townhall: Join us as we reflect on the election results, breathe, and share strategy as we enter 2025. Guests will include leaders from Center for Constitutional Rights, Black Voters Matter, Working Families Party, Abolitionist Law Center, Mass Liberation Arizona. Black Alliance for Just Immigration, and Movement for Black Lives. Sign up for Thu 11/7, 6:00 PM Eastern.</li> <li>Force Multiplier\u2019s Election Reflection: Whatever we know or don\u2019t know by that time, we\u2019re sure it will be better if we're together. Let's look at our wins and our losses and talk about where we are and where we need to go. Sign up for Thu 11/7, 8:30 PM Eastern.</li> <li>Power to the People: Where We Go From Here \u2014 Join the Workers Circle for a post-election discussion featuring Ari Berman of People For the American Way; courts expert Marge Baker; and Andrea Miller, founder of the Center for Common Ground. We'll explore the political landscape and our role in defending democracy, voting rights, and court reform. Sign up, ask questions, and connect with activists nationwide on Tue 11/12, 7:00 PM Eastern.</li> </ul>"},{"location":"other/2024-election-ending/#articles","title":"Articles","text":"<ul> <li>\u201cRed Wave\u201d Redux: Are GOP Polls Rigging the Averages in Trump\u2019s Favor? (New Republic): Polling by right-leaning firms has exploded this cycle. It could be that they\u2019re trying to create a (false) sense of momentum for Donald Trump.</li> <li>Our Elections Are Not Fragile: This article from the Brennan Center highlights two large trends in our elections. First, election deniers have ramped up their attacks. Second, despite these attacks, our election system is stronger than it was in 2020 and better equipped to repel these attacks.</li> <li> <p>5 Days Out I Would Much Rather Be Us Than Them - New 2024 Election Analysis and Video With Tom Bonier (Hopium Chronicles) Excerpts:</p> <p>R[epublican]s would only be working so hard to shape the national narrative to make it look like Trump is winning if they believed he wasn\u2019t.</p> <p>Dems are matching our 2020 results in the battlegrounds despite the current battleground electorate being older, whiter, more male and more rural than at this point in 2020. This is really good. Next, as we expect both the R[epublican] and unaffiliated vote to be more Democratic this time, it is very likely we are today running ahead of 2020 in the 7 battleground states. This too is really good.</p> </li> <li> <p>Why voting is a chess move, not a love letter:  a two-minute video from Maurice Mitchell of the Working Families Party.</p> </li> </ul>"},{"location":"other/2024-election/","title":"2024 Election","text":"<p>Here are some ways to take action this election:</p>"},{"location":"other/2024-election/#donate","title":"Donate","text":"<ul> <li>Donate to the Harris Walz campaign at kamalaharris.com</li> <li>Donate to Movement Voter Project PAC to help move millions in funding to local grassroots groups turning out the hardest-to-reach voters who can tip the scales in the places that matter most.</li> <li>Support Force Multiplier's House Impact Slate, Senate Slate, and Blue Surge Turnout Fund at forcemultiplierus.org. They're an all-volunteer group that provides donors with research-based recommendations on which races are most likely to affect the balance of power in the House + Senate. NOTE: They do not screen candidates on issues.</li> </ul>"},{"location":"other/2024-election/#take-action","title":"Take action","text":"<ul> <li>Visit Swing Blue Alliance \u2014 SBA is the largest all-volunteer, Democratic grassroots organization headquartered in Massachusetts. They have all kinds of actions \u2014 phone banking, canvassing, texting, postcarding, letter writing, etc. \u2014 so you can find what works best for you. <ul> <li>Janelle Stelson, PA-10 \u2014 Call voters with Massachusetts\u2019 own Swing Blue Alliance to elect Pennsylvania Democrat Janelle Stelson to Congress, fire MAGA Republican Congressman Scott Perry, and Flip the House. Pennsylvania is a battleground state critical to defeating Trump. Sign up for calling on Thursdays or Sundays!</li> </ul> </li> <li>New Hampshire \u2014 Join other Massachusetts volunteers for door knocking in Nashua, NH or door knocking in Manchester, NH. I\u2019ve heard that New Hampshire\u2019s local and congressional races are important and not a sure thing, and that New Hampshire needs the help from Massachusetts volunteers.<ul> <li>Chris Messinger (formerly with TASC) is organizing folks to door knock in Congressional District 2 where there\u2019s no incumbent. Most door knocked on behalf of Maggie Goodlander. This is organized out of someone\u2019s house and going to Salem, Windham, Pelham. They\u2019re really grateful to have the support of volunteers from Massachusetts. This is a much more red district than Manchester, so probably more useful.</li> </ul> </li> <li>Join Bend the Arc for remote volunteering opportunities:<ul> <li>Reach out to voters in Wisconsin to do non-partisan vote planning before the November election. Sundays in October/November. Details and sign up here.</li> <li>Join Bend the Arc: Jewish Action and One Arizona to call voters in the state before the general election. Details and sign up here.</li> <li>Jews for Kamala Harris Phonebank \u2014 details and sign up.</li> </ul> </li> <li>Go door knocking in swing states with Seed The Vote \u2014 I've heard from a couple of people who are volunteering with Seed The Vote in Nevada and Michigan. You can read this blog post about a weekend volunteering with STV in Reno, Nevada from someone I know. As Jeff writes: <p>STV most needs volunteers in the critical swing states of Arizona, Michigan, Wisconsin, &amp; Pennsylvania. We're knocking in all those states now through election day. For those who need financial support, STV can often help cover hotel &amp; flight costs. To learn more, sign up for an info session here.</p> </li> <li>Write letters with Vote Forward \u2014 Vote Forward is aiming to send 10 million letters this year. Since 2018, their 269,000 volunteers have sent 35.5 million letters, which have been shown to boost turnout by up to 3.4%. Their nonpartisan arm encourages potential voters to register to vote and increase participation of historically underrepresented communities. Their 501(c)(4) arm helps increase voter turnout in political campaigns. I\u2019ve sent letters with them in the past and you don\u2019t need fancy supplies, other than stamps, envelopes, and access to a printer. Learn more and sign up at votefwd.org.</li> <li>Join non-partisan or partisan text banking: registration and get out the vote text banking. Exercise your rights text banking.<ul> <li>Next Gen Voters - organizing.nextgenamerica.org </li> <li>Field Team 6 (a consortium of groups) - www.fieldteam6.org</li> <li>Reproductive Freedom for All (formerly called NARAL) - reproductivefreedomforall.org </li> </ul> </li> <li>Poll watching training for New Hampshire: Please join us for one of our online training (multiple dates) to be a poll observer in New Hampshire on Election Day\u00a0</li> </ul>"},{"location":"other/2024-election/#massachusetts-ballot-questions","title":"Massachusetts Ballot Questions","text":"<p>October 9, 6:30 PM Eastern:\u00a0Learn more about the ballot questions, a training by Asian Pacific Islanders Civic Action Network</p>"},{"location":"other/2024-election/#question-2-eliminating-mcas-graduation-requirement","title":"Question 2: Eliminating MCAS graduation requirement","text":"<ul> <li>Join canvassing here: www.yesonquestion2ma.com/join </li> </ul>"},{"location":"other/do-something/","title":"\u201cDo something\u201d with Seth &amp; Rina \u2014 Sun Oct 27","text":"<p>Seth &amp; Rina are hosting an election-related party: to influence the election and be in community together to nourish our democracy.\u00a0</p> <ul> <li>Sunday, October 27th, 1:30 - 5:00 PM: phone banking, letter writing, and happy hour (see below for details)</li> <li>Location in Arlington will be provided after you RSVP</li> </ul>"},{"location":"other/do-something/#please-rsvp-at-this-link-as-soon-as-possible","title":"Please RSVP at this link as soon as possible.","text":"<p>We need an estimate of the participants by Friday Oct 25th to have enough materials and refreshments.</p>"},{"location":"other/do-something/#phone-banking-130-400","title":"Phone Banking (1:30-4:00)","text":"<p>We will join a phone bank hosted by the Democratic National Committee to make calls for Kamala Harris and Democrats up and down the ballot in swing states. These days, phone banks use Zoom plus your phone, the Scale To Win autodialer (the best autodialer!) to make the most efficient use of your time, and a script and instructions provided by the campaign. Your phone number will not be shown to callers, and you only use your first name with those whom you\u2019re calling. Note that we're not expert phonebankers or trainers, so if you plan to phone bank, please:</p> <ul> <li>Show up by 1:40 to get set up, connect to WiFi, make sure your Zoom is up to date, etc.</li> <li>Bring your own phone, laptop, and headphones.</li> <li>Register directly with the DNC by signing up here for the Oct 27th shift from 2-4 PM.</li> <li>We encourage you to have done a previous phonebank (available every day of the week, 6-8 PM on weekdays and 2-4 PM on weekends). But no worries if you haven\u2019t \u2014 first timers welcome!</li> <li>If Sunday the 27th is your first phonebank, there will be an on online training over Zoom at 2:00 from the DNC that will take about 45 minutes.</li> </ul>"},{"location":"other/do-something/#letter-writing-200-400","title":"Letter Writing (2:00-4:00)","text":"<p>We'll most likely write letters provided by Vote Forward, to Pennsylvania or one of the tight Congressional races in New York.</p> <p>If you have time in advance, please read the following: </p> <ul> <li>Vote Forward's approach (no need to create an account or print anything)</li> <li>The \"Prepare Your Letters\" section on their\u00a0Instructions page</li> <li>\"Volunteer Training\"\u00a0for tips on how to write a good letter with your own personal story </li> </ul> <p>But if you don\u2019t have time to prepare, no worries \u2014 we\u2019ll have everything here for you.</p>"},{"location":"other/do-something/#democracy-happy-hour-400-500","title":"Democracy Happy Hour (4:00-5:00)","text":"<p>Hang out and celebrate our hard work to support our democracy. We'll have drinks and snacks, and we'd love to hang out and chat!</p> <p>This page was lovingly copied and adapted from a similar event that our friends Jeff and Kim are hosting in California.</p>"},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/archive/2018/","title":"2018","text":""},{"location":"blog/category/data-science/","title":"data science","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/category/data-science/page/2/","title":"data science","text":""}]}